{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918afbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294891b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists to list\n",
    "def flatten(itemlist):\n",
    "    return list(chain.from_iterable(itemlist))\n",
    "\n",
    "# remove symbols\n",
    "def remove_symbols(entry):\n",
    "    entry = re.sub(r'[^A-Za-z]', ' ', entry)\n",
    "    return entry\n",
    "\n",
    "# remove single letter\n",
    "def remove_char(entry): \n",
    "    entry = re.sub(r'\\b\\w\\b', '', entry)\n",
    "    return entry\n",
    "\n",
    "# sort on length\n",
    "# df.sort(key=len, reverse=False)\n",
    "\n",
    "# clean data (to lowercase, remove links, symbols, and stop words, stemming or lemmatization, tokenization)\n",
    "def clean_data(df, process):\n",
    "    df = [re.sub('@[A-Za-z0-9_]+', '', entry) for entry in df]\n",
    "    df = [re.sub('#[A-Za-z0-9_]+', '', entry) for entry in df]\n",
    "    df = [re.sub('https?://\\S+', '', entry) for entry in df]\n",
    "    df = [re.sub('(htt|RT)', '', entry) for entry in df]\n",
    "    df = [entry.lower() for entry in df]\n",
    "    df = [remove_symbols(entry) for entry in df]\n",
    "    df = flatten([word_tokenize(entry) for entry in df])\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df = [entry for entry in df if entry not in stop_words]    \n",
    "\n",
    "    if (process == 'stem'):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        df = [stemmer.stem(entry) for entry in df]\n",
    "\n",
    "    elif (process == 'lemma'):\n",
    "        word_lemmatized = WordNetLemmatizer()     \n",
    "        df = [word_lemmatized.lemmatize(entry) for entry in df]\n",
    "        \n",
    "    df = [remove_char(entry) for entry in df]\n",
    "\n",
    "    return df\n",
    "\n",
    "# WordCloud\n",
    "def wordy_cloud(df):\n",
    "    print('The number of words in the document is', len(df))\n",
    "    long_string = \" \".join(list(df))\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000,\n",
    "            width=900, height=450, contour_width=3, contour_color='steelblue', random_state=None)\n",
    "    wordcloud.generate(long_string)\n",
    "    display(wordcloud.to_image())\n",
    "\n",
    "# complete process\n",
    "def complete_process(PATH, filename, process):\n",
    "    df = pd.read_csv(os.path.join(PATH, filename), encoding='utf-8')\n",
    "    df = df.sample(frac=0.01, replace=False, random_state=None).reset_index(drop=True)\n",
    "    df = df.iloc[:, 1] # select the appropriate column based on location\n",
    "    df2 = clean_data(df, process)\n",
    "    wordy_cloud(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb692f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/tweets_data/'\n",
    "filename = 'twitter_sentiment_data'\n",
    "complete_process(PATH, f'{filename}.csv', process='stem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9151c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_process(PATH, f'{filename}.csv', process='lemma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
